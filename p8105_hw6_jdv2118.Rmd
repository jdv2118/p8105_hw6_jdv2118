---
title: "p8105_hw6_jdv2118"
author: "Justin Vargas"
output: github_document
---

# Loading Libraries and Setting Graphic Settings

```{r, message = FALSE}
library(tidyverse)

library(viridis)

library(modelr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d

scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 

# Problem 2

## Cleaning and Wrangling Data

```{r, message = FALSE, warning = FALSE}
homicide_data = 
  read_csv("data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    result = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Closed by arrest"      ~ 1,
      disposition == "Open/No arrest"        ~ 0
      ),
    victim_age = as.numeric(victim_age)
  ) %>% 
  filter(city_state != "Dallas, TX",
         city_state != "Phoenix, AZ",
         city_state != "Kansas City, MO",
         city_state != "Tulsa, AL",
victim_race %in% c("White", "Black"),
victim_sex != "Unknown") %>% 
  select(city_state, result, victim_age, victim_race, victim_sex)

head(homicide_data)
```

The code above is used to clean and wrangle the data.

## Fitting Logistic Regression For Baltimore, MD

```{r}
baltimore_glm =
  homicide_data %>% 
  filter(city_state == "Baltimore, MD")

glm(result ~ victim_age + victim_sex + victim_race, 
    data = baltimore_glm,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    or = exp(estimate),
    ci_lower = exp(estimate - 1.96 * std.error),
    ci_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, or, starts_with("ci")) %>% 
  knitr::kable(digits = 3)
```

The code above is used to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors for the city of Baltimore, MD.

## Fitting Logistic Regression For Each City

```{r}
cities_glm = 
  homicide_data %>% 
  nest(data = -city_state) %>% 
  mutate(
    city_models = 
      map(.x = data, ~glm(result ~ victim_age + victim_sex + victim_race, data = .x, family = binomial())),
    result = map(city_models, broom::tidy)
  ) %>% 
  select(city_state, result) %>% 
  unnest(result) %>% 
  mutate(
    OR = exp(estimate),
    ci_lower = exp(estimate - 1.96 * std.error),
    ci_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("ci")) %>%
  filter(term == "victim_sexMale")
  
cities_glm
```

The code above is used to fit the same logistic regression for each city in the dataset.

## Plot of ORs and CIs for Each City

```{r}
cities_plot =
  cities_glm %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) + 
  labs(
    x = "City and State Names",
    y = "Odds Ratio",
    title = "The Odds Ratio for Each City and State"
  ) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

cities_plot
```

Based on the plot, New York, NY, has the lowest odds ratio, while Albuquerque, NM, has the highest odds ratio.  It is important to note that Albuquerque, NM, appears to have the widest confidence interval, while Chicago, IL, has the smallest confidence interval.  Ultimately, there is overlap among the confidence intervals for the various cities.

# Problem 3

## Loading and Cleaning The Data

```{r, message = FALSE}
birthweight_data =
  read_csv("data/birthweight.csv") %>%
  mutate(
    babysex = as.factor(babysex),
    babysex = recode(babysex, "1" = "Male", "2" = "Female"),
    frace = as.factor(frace),
    frace = recode(frace, "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other",      "9" = "Unknown"),
    malform = as.factor(malform),
    malform = recode(malform, "0" = "Absent", "1" = "Present"),
    mrace = as.factor(mrace),
    mrace = recode(mrace, "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other")
  )

head(birthweight_data)

sum(is.na(birthweight_data))
```

The code above is used to load and clean the data.  There is no missing data.

## My Proposed Model

```{r}
birthweight_model = lm(bwt ~ babysex + fincome + gaweeks + malform + smoken + wtgain, data = birthweight_data)

birthweight_model
```

My proposed model consists of the following variables: babysex, fincome, gaweeks, malform, smoke, and wtgain.  I hypothesize that these variables underly birthweight, hence why they were chosen for my proposed model. 

## Plot of My Proposed Model

```{r}
birthweight_plot =
birthweight_data %>% 
  modelr::add_predictions(birthweight_model) %>% 
  modelr::add_residuals(birthweight_model) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .2) +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    title = "Residuals vs Fitted Values"
  )

birthweight_plot
```

The code above is used to create a plot of model residuals against fitted values for my proposed model.

```{r}
set.seed(1)

cross_validation = 
  crossv_mc(birthweight_data, 100) %>% 
  mutate(
    model_1 = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_2 = map(.x = train, ~lm(bwt ~ bhead * blength * babysex, data = .x)),
    my_model = map(.x = train, ~lm(bwt ~ babysex + fincome + gaweeks + malform + smoken + wtgain, data = .x))
  ) %>% 
  mutate(
    rmse_model_1 = map2_dbl(.x = model_1, .y = test, ~rmse(model = .x, data = .y)),
    rmse_model_2 = map2_dbl(.x = model_2, .y = test, ~rmse(model = .x, data = .y)),
    rmse_my_model = map2_dbl(.x = my_model, .y = test, ~rmse(model = .x, data = .y))
  )

cross_validation

cross_validation_plot =
 cross_validation %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  mutate(
    model = recode(model, "model_1" = "Model 1", "model_2" = "Model 2", "my_model" = "My Model")
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(
    x = "Model",
    y = "RMSE",
    title = "RMSE by Model"
  )

cross_validation_plot
```

Based on the plot above, Model 2, which consists of the head circumference, length, sex, and all interactions, had the lowest RMSE of all of the models including my proposed model.  This indicates that it is the best model of all of the other models.  Model 1, which consists of length at birth and gestational age, has a RMSE that is greater than that of Model 2, but less than that of my proposed model.  This indicates that Model 2 is the second best model.  My proposed model has the highest RMSE, which indicates that it is the worst model of all the models. 

